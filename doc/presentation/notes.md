Good morning everybody. I am Nikhil and I worked on a project related to
the semantic web on distributed systems. Precisely - Semantic mapping of XML
metadata for Cloud based Large Scale Data Management.

Change

So to start of, what has happened on the Internet in the last decade is that
a lot of information that is being produced is becoming more useful to other
computers or other programs that can consume that information, rather than only
humans reading it. To give the most visible example, Facebook integrates with
a lot of websites and updates your timeline with information about songs you've
been listening too, articles you've read and other activities you do online.
Similarly, intelligent agents try to learn from your digital activities.
A large quantity of information is generated by various sensors like weather
data, GPS data, seismological data.

The semantic web is based around the fact that if we can manage to
describe all information we produced in concrete terms, with enforced
relationships and no ambiguity, then machines will be able to handle the
information well and produce some very interesting results. The semantic web
provides a common framework that allows data to be shared and reused across
application, enterprise and community boundaries. It is based around the
Resource Description Framework, a way of modeling information.

Unfortunately, despite the great success the semantic web has enjoyed in theory
and in initiatives like Linked Data, most of the top information producing
systems are not using semantically expressed data, but rather their own custom
formats, where the meaning has to be explicitly documented for every service.

Change

In the future we want everyone using data with attached semantic meaning so
that computers can freely exchange data.

Change

But till that vision is a reality, we will need to keep dealing with multiple
sources. What if we could build 'gateways' of sorts, which converted these
custom data formats, to valid semantic data? This way consumers could already
start taking advantage of converted RDF data.

There are two problems that I have tried to tackle in the project.

Change

The first is converting custom XML data to well-structured, unique RDF that
tries to capture the same information as well as possible. For example we see
here that the Record "OK Computer" is made by a band called Radiohead. And then
later in the XML document, we see that Radiohead is a artist who has also made
the track called Paranoid Android. It is very easy for a human to connect these
3 things and say that both instances of Radiohead are the same artist. An ideal
RDF conversion of this data, which captures the intuitive notions is shown on
the right. The problem is that there are multiple ways to write the same
description in XML 

(change)

as in here. because XML has no way to establish
associations between items, that too is something that has to be inferred.

Change

The second problem is that there is no dearth of data on the Internet, and new
data is always being produced. So just having a solution to problem 1 that can
run on a single machine is not enough, since it won't handle the load. While
dealing with tons of data, it also has to remain fast, and be resistant to
software or hardware failures.

Change

There has been significant research to convert XML to RDF, but these approaches
require some template specifying the conversion for each type. A federated
distributed semantic database has been attempted by many, using various tools
like Cassandra, Hadoop and MapReduce. YARS2 is particularly notable for having
very efficient, searching over immense amounts of semantic data in
a distributed configuration.

Change

For the scope of the project, I tried to convert XML to RDF on assumption that
the XML used concepts from some ontology, rather than being completely free
form. An ontology is a set of definitions about some domain of knowledge.

I also propose a way to cache information called the Clusterspace, obtained
from various ontologies about concepts and properties so that it can be
efficiently processed.

Finally the converted semantic data is stored in Apache Cassandra for
large scale, cloud based storage.

Change

The clusterspace is a graph that is created by using a OWL reasoner over an
ontology. It allows reasoning information about different ontologies to be
centralized, and changes in the graph can be shared quickly with other nodes in
the network.

Change

This is an example clusterspace, based on which let us see how incoming XML
data can be processed

Change

Consider these fragments describing a Record. Each of these say their title is
"OK Computer" And we see that in the Clusterspace, their is indeed such an edge
between Record and string. So we add a RDF triple - a subject, verb, object
that says that 'their is a record, we will assign id1, that has title OK
Computer'

Change

Similarly we can have object properties, which refer to another instance of
data. Again we make use of the Clusterspace, and check whether MusicArtist is
truly a Person (it is), and that a 'maker' relationship can exist between the
two. We've now adopted some more RDF triples.

Change

Sometimes in XML, their is just a tag with text content, that is acting as
a 'reference' to the actual content. This too is one of the cases handled by
the system. For this it assumes that properties which are so queried will have
that scalar value (a name, or string or number) unique.

Change

There can arise confusion, when only the concepts are mentioned without the
property being stated. In this case, the current system just picks the first
one.

Change

Change

The RDF obtained from converting the XML is stored in the following manner.

Talk

Change

Talk

Change

Let us now see the system from the consumer view, when a SPARQL query is
executed.

Explain query

Change

explain

change

explain

change

explain

change

explain

change

explain

Change 

talk about evaluation

change

Time for clusterspace formation is roughly linear with number of concepts

change

The time for XML to RDF conversion is variable as shown. The same reference RDF
file was converted to XML and then back to observe these results. Random
structural variations lead to change in performance, based on levels of
nesting.

Change

As shown, the only case where the system can create bad RDF triples is when the
property is not mentioned and a wrong guess occurs. The rest of the cases, no
erroneous results are produced, although it is possible to miss certain data
entirely. For the LUBM dataset, recall was still over 50% on 3 out of the
4 queries.

Change

The advantage of the Clusterspace is evident in the time for query processing
over the clusterspace, with graph traversals for even complex queries taking
less than a second and not requiring the DL reasoner to be invoked

Change

One of the underwhelming results was actually using a 4 node Cassandra setup to
store data. This turned out to be much slower than a single Cassandra node. The
cause was that since query processing was still centralized, all basic triples
had to be transferred to that machine, leading to a lot of slow network I/O. 

Change

Conclude

Change

There are many fruitful areas of research from which the system I've described
can benefit. ...

Change

Change

Thank you
